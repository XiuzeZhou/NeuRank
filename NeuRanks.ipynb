{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from pylab import *\n",
    "from data import *\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import auc,roc_auc_score,precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuRanks():\n",
    "    def __init__(self,               \n",
    "                 drugs_num = None, #用户数\n",
    "                 targets_num = None, #商品数\n",
    "                 S_d = None, # 药物之间的相似度\n",
    "                 S_t = None, # 靶之间的相似度\n",
    "                 batch_size = 64, #batch大小\n",
    "                 embedding_size = 64, # 嵌入空间维度\n",
    "                 hidden_size = [32,16], #隐层节点数目\n",
    "                 learning_rate = 1e-3, #学习率\n",
    "                 lamda_regularizer = 1e-5, #正则项系数\n",
    "                 lamda_regularizer_d = 0.1, #药相似度系数\n",
    "                 lamda_regularizer_t = 0.1 #靶相似度系数\n",
    "                ):\n",
    "        self.drugs_num = drugs_num\n",
    "        self.targets_num = targets_num\n",
    "        self.S_d = S_d\n",
    "        self.S_t = S_t\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lamda_regularizer = lamda_regularizer\n",
    "        self.lamda_regularizer_d= lamda_regularizer_d\n",
    "        self.lamda_regularizer_t = lamda_regularizer_t\n",
    "\n",
    "        # loss records\n",
    "        self.train_loss_records = []   \n",
    "        self.build_graph()    \n",
    "\n",
    "        \n",
    "    def build_graph(self):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            #tf.set_random_seed(-1)\n",
    "            # _________ input data _________\n",
    "            self.drugs_inputs = tf.placeholder(tf.int32, shape = [None], name='drugs_inputs')\n",
    "            self.targets_inputs = tf.placeholder(tf.int32, shape = [None], name='targets_inputs')\n",
    "            self.train_labels = tf.placeholder(tf.float32, shape = [None], name='train_labels') \n",
    "            self.S_d = tf.convert_to_tensor(self.S_d, tf.float32)\n",
    "            self.S_t = tf.convert_to_tensor(self.S_t, tf.float32)\n",
    "            \n",
    "            # _________ variables _________\n",
    "            self.weights = self._initialize_weights()\n",
    "            \n",
    "            # _________ train _____________\n",
    "            self.y_ = self.inference(drugs_inputs=self.drugs_inputs, targets_inputs=self.targets_inputs)\n",
    "            self.loss_train = self.loss_function(true_labels=self.train_labels, \n",
    "                                                 predicted_labels=tf.reshape(self.y_,shape=[-1]),\n",
    "                                                 lamda_regularizer=self.lamda_regularizer,\n",
    "                                                 lamda_regularizer_d = self.lamda_regularizer_d,\n",
    "                                                 lamda_regularizer_t = self.lamda_regularizer_t)\n",
    "            self.train_op = tf.train.AdamOptimizer(learning_rate=self.learning_rate,beta1=0.9, beta2=0.999, epsilon=1e-08).minimize(self.loss_train) \n",
    "\n",
    "            # _________ prediction _____________\n",
    "            self.predictions = self.inference(drugs_inputs=self.drugs_inputs, targets_inputs=self.targets_inputs)\n",
    "        \n",
    "            #变量初始化 init\n",
    "            self.saver = tf.train.Saver() #  \n",
    "            init = tf.global_variables_initializer()\n",
    "            self.sess = self._init_session()\n",
    "            self.sess.run(init)\n",
    "    \n",
    "    \n",
    "    def _init_session(self):\n",
    "        # adaptively growing memory\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        return tf.Session(config=config)\n",
    "    \n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        all_weights = dict()\n",
    "\n",
    "        # -----embedding layer------\n",
    "        all_weights['embedding_drugs'] = tf.Variable(tf.random_normal([self.drugs_num, self.embedding_size], 0, 0.1),name='embedding_drugs')\n",
    "        all_weights['embedding_targets'] = tf.Variable(tf.random_normal([self.targets_num, self.embedding_size], 0, 0.1),name='embedding_targets') \n",
    "        \n",
    "        # ------hidden layer------\n",
    "        all_weights['weight_0'] = tf.Variable(tf.random_normal([self.embedding_size,self.hidden_size[0]], 0.0, 0.1),name='weight_0')\n",
    "        all_weights['bias_0'] = tf.Variable(tf.zeros([self.hidden_size[0]]), name='bias_0')\n",
    "        all_weights['weight_1'] = tf.Variable(tf.random_normal([self.hidden_size[0],self.hidden_size[1]], 0.0, 0.1), name='weight_1')\n",
    "        all_weights['bias_1'] = tf.Variable(tf.zeros([self.hidden_size[1]]), name='bias_1')\n",
    "        #all_weights['weight_2'] = tf.Variable(tf.random_normal([self.hidden_size[1],self.hidden_size[-1]], 0.0, 0.1), name='weight_2')\n",
    "        #all_weights['bias_2'] = tf.Variable(tf.zeros([self.hidden_size[-1]]), name='bias_2')\n",
    "        \n",
    "        # ------output layer-----\n",
    "        all_weights['weight_n'] = tf.Variable(tf.random_normal([self.hidden_size[-1], 1], 0, 0.1), name='weight_n')\n",
    "        all_weights['bias_n'] = tf.Variable(tf.zeros([1]), name='bias_n')\n",
    "\n",
    "        return all_weights\n",
    "        \n",
    "    \n",
    "    def train(self, data_sequence):\n",
    "        train_size = len(data_sequence)\n",
    "        np.random.shuffle(data_sequence)\n",
    "        batch_size = self.batch_size\n",
    "        total_batch = math.ceil(train_size/batch_size)\n",
    "\n",
    "        for batch in range(total_batch):\n",
    "            start = (batch*batch_size)% train_size\n",
    "            end = min(start+batch_size, train_size)\n",
    "            data_array = np.array(data_sequence[start:end])\n",
    "            X = data_array[:,:2] # u,i\n",
    "            y = data_array[:,-1] # label\n",
    "\n",
    "            loss_val=self.fit(X=X, y=y)\n",
    "            self.train_loss_records.append(loss_val)\n",
    "            \n",
    "        return self.train_loss_records\n",
    "\n",
    "        \n",
    "    # 网络的前向传播\n",
    "    def inference(self, drugs_inputs, targets_inputs):\n",
    "        embed_drugs = tf.reshape(tf.nn.embedding_lookup(self.weights['embedding_drugs'], drugs_inputs),\n",
    "                                 shape=[-1, self.embedding_size])\n",
    "        embed_targets = tf.reshape(tf.nn.embedding_lookup(self.weights['embedding_targets'], targets_inputs),\n",
    "                                 shape=[-1, self.embedding_size])\n",
    "            \n",
    "        layer0 = tf.nn.relu(tf.matmul(embed_targets*embed_drugs, self.weights['weight_0']) + self.weights['bias_0'])\n",
    "        layer1 = tf.nn.relu(tf.matmul(layer0, self.weights['weight_1']) + self.weights['bias_1']) \n",
    "        #layer2 = tf.nn.relu(tf.matmul(layer1, self.weights['weight_2']) + self.weights['bias_2'])  \n",
    "        y_ = tf.matmul(layer1,self.weights['weight_n']) + self.weights['bias_n']\n",
    "        return y_         \n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # X: 输入数据\n",
    "        # y: 输入标签\n",
    "        feed_dict = {self.drugs_inputs: X[:,0], self.targets_inputs: X[:,1],self.train_labels:y}  \n",
    "        loss, opt = self.sess.run([self.loss_train,self.train_op], feed_dict=feed_dict)\n",
    "        return loss\n",
    "        \n",
    "        \n",
    "    def loss_function(self, true_labels, predicted_labels,lamda_regularizer=1e-5, lamda_regularizer_d=1e-6, lamda_regularizer_t=1e-6):   \n",
    "        rmse = tf.losses.mean_squared_error(true_labels, predicted_labels)\n",
    "        regularizer_1 = tf.contrib.layers.l2_regularizer(lamda_regularizer)\n",
    "        regularization_1 = regularizer_1(\n",
    "            self.weights['embedding_drugs']) + regularizer_1(\n",
    "            self.weights['embedding_targets'])+ regularizer_1(\n",
    "            self.weights['weight_0']) + regularizer_1(\n",
    "            self.weights['weight_1']) + regularizer_1(\n",
    "            self.weights['weight_n'])\n",
    "        \n",
    "        drug1 = tf.reshape(tf.tile(self.weights['embedding_drugs'],(1,self.drugs_num)),shape=[-1, self.embedding_size])\n",
    "        drug2 = tf.tile(self.weights['embedding_drugs'],(self.drugs_num,1))\n",
    "        sim_d = tf.reshape(tf.reduce_sum(drug1*drug2, axis=1), shape=[-1,self.drugs_num])\n",
    "        s_score = tf.reshape(sim_d, shape=[-1])\n",
    "        s_true = tf.reshape(self.S_d, shape=[-1])\n",
    "        regularization_2 = lamda_regularizer_d * tf.losses.mean_squared_error(s_true, s_score)\n",
    "        \n",
    "        target1 = tf.reshape(tf.tile(self.weights['embedding_targets'],(1,self.targets_num)),shape=[-1, self.embedding_size])\n",
    "        target2 = tf.tile(self.weights['embedding_targets'],(self.targets_num,1))\n",
    "        sim_t = tf.reshape(tf.reduce_sum(target1*target2, axis=1), shape=[-1,self.targets_num])\n",
    "        s_score = tf.reshape(sim_t, shape=[-1])\n",
    "        s_true = tf.reshape(self.S_t, shape=[-1])\n",
    "        regularization_3 = lamda_regularizer_t * tf.losses.mean_squared_error(s_true, s_score)\n",
    "        \n",
    "        cost = rmse + regularization_1 + regularization_2 + regularization_3\n",
    "        return cost   \n",
    "        \n",
    "        \n",
    "    def evaluate(self, X, labels):\n",
    "        drugs_inputs = X[:,0]\n",
    "        targets_inputs = X[:,1]\n",
    "        feed_dict = {self.drugs_inputs: drugs_inputs, self.targets_inputs: targets_inputs}  \n",
    "        score = self.sess.run([self.predictions], feed_dict=feed_dict)       \n",
    "        y_pred = np.reshape(score,(-1))\n",
    "        \n",
    "        auc_score = roc_auc_score(labels, y_pred)\n",
    "        precision, recall, pr_thresholds = precision_recall_curve(labels, y_pred)\n",
    "        aupr_score = auc(recall, precision)\n",
    "        return auc_score, aupr_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(train_mat, sample_size=4):\n",
    "    data = []\n",
    "    drugs_num,targets_num = train_mat.shape\n",
    "    for d in range(drugs_num):\n",
    "        positive_targets = np.where(train_mat[d,:]>0)[0] #drug 中大于零的项\n",
    "        \n",
    "        for target0 in positive_targets:\n",
    "            data.append([d,target0,1])\n",
    "            i = 0\n",
    "            while i<sample_size:\n",
    "                target1 = np.random.randint(targets_num)\n",
    "                if abs(train_mat[d,target1])<1e-6:\n",
    "                    data.append([d,target1,0])\n",
    "                    i = i+1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train for drug-target pairs\n",
    "\n",
    "**Enzymes:**\n",
    "\n",
    "- AUC\n",
    "\n",
    "embedding_size = 32 # 用户的嵌入空间维度\n",
    "\n",
    "hidden_size = [16,8] #隐层节点数目\n",
    "\n",
    "learning_rate = 0.005 #学习率\n",
    "\n",
    "lamda_regularizer = 1e-5 #正则项系数\n",
    "\n",
    "lamda_regularizer_d = 0.1 #正则项系数 \n",
    "\n",
    "lamda_regularizer_t = 0.1 #正则项系数\n",
    "\n",
    "if loss_records[-1] < 0.05 and len(mode_list) > 2 and mode_list[-3] > mode_list[-1] and mode_list[-3] > mode_list[-2]:\n",
    "\n",
    "train(data_list=data_list, S_d=S_d, S_t=S_t, drugs_num=drugs_num, targets_num=targets_num, mode='auc')\n",
    "\n",
    "- AUPR\n",
    "\n",
    "embedding_size = 32 # 用户的嵌入空间维度\n",
    "\n",
    "hidden_size = [16,16] #隐层节点数目\n",
    "\n",
    "learning_rate = 0.0005 #学习率\n",
    "\n",
    "lamda_regularizer = 1e-4 #正则项系数\n",
    "\n",
    "lamda_regularizer_d = 1. #正则项系数 \n",
    "\n",
    "lamda_regularizer_t = 1. #正则项系数 \n",
    "\n",
    "if loss_records[-1] < 0.1 and len(mode_list) > 10 and mode_list[-3] > mode_list[-1] and mode_list[-3] > mode_list[-2]:\n",
    "\n",
    "train(data_list=data_list, S_d=S_d, S_t=S_t, drugs_num=drugs_num, targets_num=targets_num, mode='aupr')\n",
    "\n",
    "**Ion Channels:**\n",
    "\n",
    "- AUC\n",
    "\n",
    "embedding_size = 32 # 用户的嵌入空间维度\n",
    "\n",
    "hidden_size = [16,8] #隐层节点数目\n",
    "\n",
    "learning_rate = 0.0003 #学习率\n",
    "\n",
    "lamda_regularizer = 1e-4 #正则项系数\n",
    "\n",
    "lamda_regularizer_d = 0.01 #正则项系数 \n",
    "\n",
    "lamda_regularizer_t = 0.01 #正则项系数\n",
    "\n",
    "if loss_records[-1] < 0.04 and len(mode_list) > 10 and (mode_list[-2] < mode_list[-3] > mode_list[-1]):\n",
    "\n",
    "train(data_list=data_list, S_d=S_d, S_t=S_t, drugs_num=drugs_num, targets_num=targets_num, mode='auc')\n",
    "\n",
    "####################################################################################################\n",
    "\n",
    "embedding_size = 32 # 用户的嵌入空间维度\n",
    "\n",
    "hidden_size = [16,16] #隐层节点数目\n",
    "\n",
    "learning_rate = 0.001 #学习率\n",
    "\n",
    "lamda_regularizer = 1e-4 #正则项系数\n",
    "\n",
    "lamda_regularizer_d = 1e-4 #正则项系数\n",
    "\n",
    "lamda_regularizer_t = 1e-4 #正则项系数 \n",
    "\n",
    "if loss_records[-1] < 0.03 and len(mode_list) > 10 and (mode_list[-2] < mode_list[-3] > mode_list[-1]):\n",
    "\n",
    "train(data_list=data_list, S_d=S_d, S_t=S_t, drugs_num=drugs_num, targets_num=targets_num, mode='aupr')\n",
    "\n",
    "**GPCRs:**\n",
    "\n",
    "- AUC\n",
    "\n",
    "embedding_size = 8 # 用户的嵌入空间维度\n",
    "\n",
    "hidden_size = [4,4] #隐层节点数目\n",
    "\n",
    "learning_rate = 0.001 #学习率\n",
    "\n",
    "lamda_regularizer = 1e-4 #正则项系数\n",
    "\n",
    "lamda_regularizer_d = 0.5 #正则项系数 \n",
    "\n",
    "lamda_regularizer_t = 0.5 #正则项系数 \n",
    "\n",
    "if loss_records[-1] < 0.04 and len(mode_list) > 10 and mode_list[-3] > mode_list[-1] and mode_list[-3] > mode_list[-2]:\n",
    "\n",
    "train(data_list=data_list, S_d=S_d, S_t=S_t, drugs_num=drugs_num, targets_num=targets_num, mode='auc')\n",
    "\n",
    "####################################################################################################\n",
    "\n",
    "embedding_size = 16 # 用户的嵌入空间维度\n",
    "\n",
    "hidden_size = [8,8] #隐层节点数目\n",
    "\n",
    "learning_rate = 0.001 #学习率\n",
    "\n",
    "lamda_regularizer = 1e-5 #正则项系数\n",
    "\n",
    "lamda_regularizer_d = 0.1 #正则项系数 \n",
    "\n",
    "lamda_regularizer_t = 0.1 #正则项系数\n",
    "\n",
    "if loss_records[-1] < 0.02 and len(mode_list) > 10 and mode_list[-3] > mode_list[-1] and mode_list[-3] > mode_list[-2]:\n",
    "\n",
    "train(data_list=data_list, S_d=S_d, S_t=S_t, drugs_num=drugs_num, targets_num=targets_num, mode='aupr')\n",
    "\n",
    "####################################################################################################\n",
    "\n",
    "embedding_size = 32 # 用户的嵌入空间维度\n",
    "\n",
    "hidden_size = [16,16] #隐层节点数目\n",
    "\n",
    "learning_rate = 0.001 #学习率\n",
    "\n",
    "lamda_regularizer = 1e-3 #正则项系数\n",
    "\n",
    "lamda_regularizer_d = 1.0 #正则项系数\n",
    "\n",
    "lamda_regularizer_t = 1.0 #正则项系数\n",
    "\n",
    "if loss_records[-1] < 0.13 and len(mode_list) > 10 and mode_list[-3] > mode_list[-1] and mode_list[-3] > mode_list[-2]:\n",
    "\n",
    "train(data_list=data_list, S_d=S_d, S_t=S_t, drugs_num=drugs_num, targets_num=targets_num, mode='aupr')\n",
    "\n",
    "####################################################################################################\n",
    "\n",
    "embedding_size = 64 # 用户的嵌入空间维度\n",
    "\n",
    "hidden_size = [32,32] #隐层节点数目\n",
    "\n",
    "learning_rate = 0.001 #学习率\n",
    "\n",
    "lamda_regularizer = 1e-4 #正则项系数\n",
    "\n",
    "lamda_regularizer_d = 1e-1 #正则项系数\n",
    "\n",
    "lamda_regularizer_t = 1e-1 #正则项系数\n",
    "\n",
    "if loss_records[-1] < 0.03 and len(mode_list) > 10 and mode_list[-3] > mode_list[-1] and mode_list[-3] > mode_list[-2]:\n",
    "\n",
    "train(data_list=data_list, S_d=S_d, S_t=S_t, drugs_num=drugs_num, targets_num=targets_num, mode='aupr')\n",
    "\n",
    "- AUPR\n",
    "\n",
    "embedding_size = 32 # 用户的嵌入空间维度\n",
    "\n",
    "hidden_size = [16,16] #隐层节点数目\n",
    "\n",
    "learning_rate = 0.001 #学习率 0.0005\n",
    "\n",
    "lamda_regularizer = 1e-4 #正则项系数\n",
    "\n",
    "lamda_regularizer_d = 0.1 #正则项系数 \n",
    "\n",
    "lamda_regularizer_t = 0.1 #正则项系数 \n",
    "\n",
    "if loss_records[-1] < 0.04 and len(mode_list) > 10 and mode_list[-3] > mode_list[-1] and mode_list[-3] > mode_list[-2]:\n",
    "\n",
    "train(data_list=data_list, S_d=S_d, S_t=S_t, drugs_num=drugs_num, targets_num=targets_num, mode='aupr')\n",
    "\n",
    "**Nuclear Receptors:**\n",
    "\n",
    "- AUPR\n",
    "\n",
    "embedding_size = 64 # 用户的嵌入空间维度\n",
    "\n",
    "hidden_size = [32,16] #隐层节点数目\n",
    "\n",
    "learning_rate = 0.005 #学习率\n",
    "\n",
    "lamda_regularizer = 1e-5 #正则项系数\n",
    "\n",
    "lamda_regularizer_d = 0.1 #正则项系数 \n",
    "\n",
    "lamda_regularizer_t = 0.1 #正则项系数\n",
    "\n",
    "sample_size = 4\n",
    "\n",
    "epochs  = 256\n",
    "\n",
    "STOP CONDITION: if loss_records[-1] < 0.01 and len(mode_list) > 3 and (mode_list[-2] < mode_list[-3] > mode_list[-1]):\n",
    "\n",
    "train(data_list=data_list, S_d=S_d, S_t=S_t, drugs_num=drugs_num, targets_num=targets_num, mode='aupr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_list, S_d, S_t, drugs_num, targets_num, mode='auc'):\n",
    "    batch_size = 256 #batch大小\n",
    "    embedding_size = 32 # 用户的嵌入空间维度\n",
    "    hidden_size = [16, 8] #隐层节点数目\n",
    "    learning_rate = 0.005 #学习率\n",
    "    lamda_regularizer = 1e-5 #正则项系数\n",
    "    lamda_regularizer_d = 0.1 #正则项系数 \n",
    "    lamda_regularizer_t = 0.1 #正则项系数\n",
    "    sample_size = 4\n",
    "    epochs  = 16\n",
    "    cv = 10\n",
    "    \n",
    "    # k折划分子集\n",
    "    kf = KFold(n_splits=cv, shuffle=True)\n",
    "    data_mat = sequence2mat(sequence=data_list, N=drugs_num, M=targets_num)\n",
    "    \n",
    "    instances_list = []\n",
    "    [instances_list.append([d,t,data_mat[d,t]]) for d in range(drugs_num) for t in range(targets_num)]\n",
    "    cv_auc_list, cv_aupr_list = [],[]\n",
    "    for train_ids, test_ids in kf.split(instances_list):\n",
    "        train_list = np.array(instances_list)[train_ids]\n",
    "        test_list, test_labels = np.array(instances_list)[test_ids][:,:2], np.array(instances_list)[test_ids][:,-1]\n",
    "        train_mat = sequence2mat(sequence=train_list, N=drugs_num, M=targets_num)# train data : user-item matrix\n",
    "            \n",
    "        #创建模型\n",
    "        model = NeuRanks(drugs_num = drugs_num,\n",
    "                         targets_num = targets_num,\n",
    "                         S_d = S_d, \n",
    "                         S_t = S_t,\n",
    "                         batch_size = batch_size,\n",
    "                         embedding_size = embedding_size,\n",
    "                         hidden_size = hidden_size,\n",
    "                         learning_rate = learning_rate,\n",
    "                         lamda_regularizer=lamda_regularizer,\n",
    "                         lamda_regularizer_d = lamda_regularizer_d,\n",
    "                         lamda_regularizer_t = lamda_regularizer_t)\n",
    "        \n",
    "        auc_score, aupr_score = model.evaluate(X=np.array(test_list), labels=test_labels)\n",
    "        print('Init: AUC = %.4f, AUPR=%.4f' %(auc_score, aupr_score))\n",
    "        \n",
    "        auc_list, aupr_list = [],[]\n",
    "        auc_list.append(auc_score)\n",
    "        aupr_list.append(aupr_score)\n",
    "        for epoch in range(epochs):\n",
    "            data_sequence = generate_data(train_mat=train_mat, sample_size=sample_size)\n",
    "            loss_records = model.train(data_sequence=data_sequence)\n",
    "            auc_score, aupr_score = model.evaluate(X=np.array(test_list), labels=test_labels)\n",
    "            auc_list.append(auc_score)\n",
    "            aupr_list.append(aupr_score)\n",
    "            print('epoch=%d, loss=%.4f, AUC=%.4f, AUPR=%.4f' %(epoch,loss_records[-1],auc_score, aupr_score))\n",
    "                \n",
    "            if mode=='auc':\n",
    "                mode_list = auc_list\n",
    "            else:\n",
    "                mode_list = aupr_list\n",
    "\n",
    "            if loss_records[-1] < 0.05 and len(mode_list) > 2 and mode_list[-3] > mode_list[-1] and mode_list[-3] > mode_list[-2]:\n",
    "                cv_auc, cv_aupr = auc_list[-3], aupr_list[-3]\n",
    "                break\n",
    "            cv_auc, cv_aupr = auc_score, aupr_score\n",
    "        cv_auc_list.append(cv_auc)\n",
    "        cv_aupr_list.append(cv_aupr)\n",
    "    \n",
    "    print('AUC=%.4f, AUPR=%.4f' %(np.mean(cv_auc_list),np.mean(cv_aupr_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enzyme: N=664, M=445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files (x86)\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: FutureWarning: read_table is deprecated, use read_csv instead.\n",
      "D:\\Program Files (x86)\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: read_table is deprecated, use read_csv instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init: AUC = 0.4933, AUPR=0.0102\n",
      "epoch=0, loss=0.1639, AUC=0.7352, AUPR=0.0723\n",
      "epoch=1, loss=0.0442, AUC=0.9132, AUPR=0.5875\n",
      "epoch=2, loss=0.0213, AUC=0.9440, AUPR=0.6493\n",
      "epoch=3, loss=0.0220, AUC=0.9216, AUPR=0.6621\n",
      "epoch=4, loss=0.0104, AUC=0.8999, AUPR=0.6492\n",
      "Init: AUC = 0.4711, AUPR=0.0091\n",
      "epoch=0, loss=0.1691, AUC=0.8258, AUPR=0.1296\n",
      "epoch=1, loss=0.0649, AUC=0.9182, AUPR=0.5497\n",
      "epoch=2, loss=0.0683, AUC=0.9429, AUPR=0.6539\n",
      "epoch=3, loss=0.0247, AUC=0.9414, AUPR=0.6529\n",
      "epoch=4, loss=0.0160, AUC=0.9001, AUPR=0.6543\n",
      "Init: AUC = 0.5047, AUPR=0.0095\n",
      "epoch=0, loss=0.1419, AUC=0.7742, AUPR=0.1263\n",
      "epoch=1, loss=0.0644, AUC=0.9191, AUPR=0.5702\n",
      "epoch=2, loss=0.0404, AUC=0.9382, AUPR=0.6281\n",
      "epoch=3, loss=0.0223, AUC=0.9279, AUPR=0.6412\n",
      "epoch=4, loss=0.0207, AUC=0.9022, AUPR=0.6734\n",
      "Init: AUC = 0.5258, AUPR=0.0100\n",
      "epoch=0, loss=0.1501, AUC=0.8057, AUPR=0.0997\n",
      "epoch=1, loss=0.0893, AUC=0.9299, AUPR=0.4860\n",
      "epoch=2, loss=0.0571, AUC=0.9586, AUPR=0.6799\n",
      "epoch=3, loss=0.0294, AUC=0.9497, AUPR=0.6719\n",
      "epoch=4, loss=0.0266, AUC=0.9200, AUPR=0.6631\n",
      "Init: AUC = 0.5183, AUPR=0.0100\n",
      "epoch=0, loss=0.1088, AUC=0.8263, AUPR=0.1392\n",
      "epoch=1, loss=0.0792, AUC=0.9117, AUPR=0.5269\n",
      "epoch=2, loss=0.0367, AUC=0.9433, AUPR=0.6577\n",
      "epoch=3, loss=0.0227, AUC=0.9387, AUPR=0.6189\n",
      "epoch=4, loss=0.0192, AUC=0.9238, AUPR=0.6098\n",
      "Init: AUC = 0.5115, AUPR=0.0107\n",
      "epoch=0, loss=0.1814, AUC=0.7384, AUPR=0.0441\n",
      "epoch=1, loss=0.0942, AUC=0.9182, AUPR=0.5085\n",
      "epoch=2, loss=0.0629, AUC=0.9447, AUPR=0.6576\n",
      "epoch=3, loss=0.0258, AUC=0.9456, AUPR=0.6700\n",
      "epoch=4, loss=0.0188, AUC=0.9172, AUPR=0.6892\n",
      "epoch=5, loss=0.0129, AUC=0.8953, AUPR=0.6709\n",
      "Init: AUC = 0.4977, AUPR=0.0101\n",
      "epoch=0, loss=0.1759, AUC=0.8498, AUPR=0.2571\n",
      "epoch=1, loss=0.0973, AUC=0.9240, AUPR=0.6016\n",
      "epoch=2, loss=0.0796, AUC=0.9446, AUPR=0.6945\n",
      "epoch=3, loss=0.0234, AUC=0.9341, AUPR=0.6916\n",
      "epoch=4, loss=0.0168, AUC=0.9045, AUPR=0.6927\n",
      "Init: AUC = 0.5290, AUPR=0.0113\n",
      "epoch=0, loss=0.1461, AUC=0.7848, AUPR=0.1766\n",
      "epoch=1, loss=0.0638, AUC=0.9198, AUPR=0.6111\n",
      "epoch=2, loss=0.0399, AUC=0.9559, AUPR=0.7116\n",
      "epoch=3, loss=0.0275, AUC=0.9528, AUPR=0.7093\n",
      "epoch=4, loss=0.0117, AUC=0.9375, AUPR=0.7020\n",
      "Init: AUC = 0.5025, AUPR=0.0101\n",
      "epoch=0, loss=0.1648, AUC=0.7717, AUPR=0.0872\n",
      "epoch=1, loss=0.1347, AUC=0.9134, AUPR=0.5325\n",
      "epoch=2, loss=0.0439, AUC=0.9432, AUPR=0.6783\n",
      "epoch=3, loss=0.0518, AUC=0.9478, AUPR=0.7201\n",
      "epoch=4, loss=0.0127, AUC=0.9411, AUPR=0.7116\n",
      "epoch=5, loss=0.0186, AUC=0.9243, AUPR=0.7338\n",
      "Init: AUC = 0.5212, AUPR=0.0110\n",
      "epoch=0, loss=0.1369, AUC=0.8480, AUPR=0.2509\n",
      "epoch=1, loss=0.0450, AUC=0.9320, AUPR=0.6164\n",
      "epoch=2, loss=0.0261, AUC=0.9399, AUPR=0.6634\n",
      "epoch=3, loss=0.0185, AUC=0.9342, AUPR=0.6790\n",
      "epoch=4, loss=0.0096, AUC=0.8985, AUPR=0.6704\n",
      "AUC=0.9461, AUPR=0.6729\n",
      "---------------------------------------------------------------------------------------------\n",
      "Init: AUC = 0.5170, AUPR=0.0108\n",
      "epoch=0, loss=0.1667, AUC=0.8120, AUPR=0.1666\n",
      "epoch=1, loss=0.1227, AUC=0.9211, AUPR=0.5009\n",
      "epoch=2, loss=0.0587, AUC=0.9502, AUPR=0.5897\n",
      "epoch=3, loss=0.0154, AUC=0.9519, AUPR=0.6164\n",
      "epoch=4, loss=0.0188, AUC=0.9184, AUPR=0.6420\n",
      "epoch=5, loss=0.0140, AUC=0.9003, AUPR=0.6318\n",
      "Init: AUC = 0.5088, AUPR=0.0105\n",
      "epoch=0, loss=0.1319, AUC=0.8190, AUPR=0.1279\n",
      "epoch=1, loss=0.0930, AUC=0.9316, AUPR=0.5754\n",
      "epoch=2, loss=0.0466, AUC=0.9539, AUPR=0.6919\n",
      "epoch=3, loss=0.0257, AUC=0.9371, AUPR=0.6893\n",
      "epoch=4, loss=0.0114, AUC=0.8983, AUPR=0.6845\n",
      "Init: AUC = 0.5206, AUPR=0.0109\n",
      "epoch=0, loss=0.2062, AUC=0.8006, AUPR=0.2033\n",
      "epoch=1, loss=0.0396, AUC=0.9246, AUPR=0.5315\n",
      "epoch=2, loss=0.0612, AUC=0.9476, AUPR=0.7025\n",
      "epoch=3, loss=0.0144, AUC=0.9437, AUPR=0.7352\n",
      "epoch=4, loss=0.0093, AUC=0.9212, AUPR=0.7273\n",
      "Init: AUC = 0.5317, AUPR=0.0117\n",
      "epoch=0, loss=0.1794, AUC=0.7763, AUPR=0.1325\n",
      "epoch=1, loss=0.0715, AUC=0.8899, AUPR=0.5268\n",
      "epoch=2, loss=0.0467, AUC=0.9393, AUPR=0.6160\n",
      "epoch=3, loss=0.0339, AUC=0.9373, AUPR=0.6505\n",
      "epoch=4, loss=0.0257, AUC=0.8997, AUPR=0.6594\n",
      "Init: AUC = 0.5117, AUPR=0.0099\n",
      "epoch=0, loss=0.1285, AUC=0.7277, AUPR=0.0929\n",
      "epoch=1, loss=0.0982, AUC=0.9105, AUPR=0.5419\n",
      "epoch=2, loss=0.0352, AUC=0.9447, AUPR=0.6644\n",
      "epoch=3, loss=0.0259, AUC=0.9272, AUPR=0.7027\n",
      "epoch=4, loss=0.0187, AUC=0.9044, AUPR=0.7026\n",
      "Init: AUC = 0.4888, AUPR=0.0100\n",
      "epoch=0, loss=0.1746, AUC=0.7825, AUPR=0.1717\n",
      "epoch=1, loss=0.0722, AUC=0.9267, AUPR=0.5418\n",
      "epoch=2, loss=0.0440, AUC=0.9518, AUPR=0.7194\n",
      "epoch=3, loss=0.0500, AUC=0.9505, AUPR=0.7791\n",
      "epoch=4, loss=0.0121, AUC=0.9270, AUPR=0.7619\n",
      "Init: AUC = 0.5132, AUPR=0.0108\n",
      "epoch=0, loss=0.1649, AUC=0.7761, AUPR=0.1673\n",
      "epoch=1, loss=0.1118, AUC=0.9042, AUPR=0.5022\n",
      "epoch=2, loss=0.0347, AUC=0.9424, AUPR=0.5938\n",
      "epoch=3, loss=0.0318, AUC=0.9369, AUPR=0.6299\n",
      "epoch=4, loss=0.0293, AUC=0.8882, AUPR=0.6401\n",
      "Init: AUC = 0.4882, AUPR=0.0104\n",
      "epoch=0, loss=0.1277, AUC=0.7851, AUPR=0.2976\n",
      "epoch=1, loss=0.0821, AUC=0.9166, AUPR=0.5056\n",
      "epoch=2, loss=0.0481, AUC=0.9525, AUPR=0.6839\n",
      "epoch=3, loss=0.0597, AUC=0.9546, AUPR=0.6770\n",
      "epoch=4, loss=0.0207, AUC=0.9231, AUPR=0.7136\n",
      "epoch=5, loss=0.0068, AUC=0.8979, AUPR=0.7009\n",
      "Init: AUC = 0.4953, AUPR=0.0102\n",
      "epoch=0, loss=0.1655, AUC=0.8208, AUPR=0.1886\n",
      "epoch=1, loss=0.0898, AUC=0.9133, AUPR=0.5754\n",
      "epoch=2, loss=0.0875, AUC=0.9548, AUPR=0.6757\n",
      "epoch=3, loss=0.0782, AUC=0.9493, AUPR=0.7110\n",
      "epoch=4, loss=0.0345, AUC=0.9421, AUPR=0.7360\n",
      "Init: AUC = 0.4996, AUPR=0.0092\n",
      "epoch=0, loss=0.1801, AUC=0.7676, AUPR=0.1134\n",
      "epoch=1, loss=0.0704, AUC=0.9164, AUPR=0.5716\n",
      "epoch=2, loss=0.0603, AUC=0.9450, AUPR=0.6645\n",
      "epoch=3, loss=0.0290, AUC=0.9466, AUPR=0.6793\n",
      "epoch=4, loss=0.0175, AUC=0.9257, AUPR=0.6996\n",
      "epoch=5, loss=0.0130, AUC=0.9141, AUPR=0.7021\n",
      "AUC=0.9488, AUPR=0.6637\n",
      "---------------------------------------------------------------------------------------------\n",
      "Init: AUC = 0.5016, AUPR=0.0096\n",
      "epoch=0, loss=0.1780, AUC=0.7938, AUPR=0.1628\n",
      "epoch=1, loss=0.0953, AUC=0.9301, AUPR=0.6065\n",
      "epoch=2, loss=0.0539, AUC=0.9478, AUPR=0.6142\n",
      "epoch=3, loss=0.0293, AUC=0.9408, AUPR=0.6614\n",
      "epoch=4, loss=0.0322, AUC=0.9119, AUPR=0.6619\n",
      "Init: AUC = 0.5329, AUPR=0.0130\n",
      "epoch=0, loss=0.1697, AUC=0.7819, AUPR=0.2151\n",
      "epoch=1, loss=0.0791, AUC=0.9266, AUPR=0.6259\n",
      "epoch=2, loss=0.0474, AUC=0.9627, AUPR=0.7409\n",
      "epoch=3, loss=0.0236, AUC=0.9547, AUPR=0.7438\n",
      "epoch=4, loss=0.0229, AUC=0.9261, AUPR=0.7308\n",
      "Init: AUC = 0.4722, AUPR=0.0090\n",
      "epoch=0, loss=0.1740, AUC=0.7377, AUPR=0.1402\n",
      "epoch=1, loss=0.1223, AUC=0.9122, AUPR=0.4927\n",
      "epoch=2, loss=0.0337, AUC=0.9309, AUPR=0.6017\n",
      "epoch=3, loss=0.0152, AUC=0.9373, AUPR=0.6416\n",
      "epoch=4, loss=0.0238, AUC=0.9030, AUPR=0.6455\n",
      "epoch=5, loss=0.0105, AUC=0.8910, AUPR=0.6431\n",
      "Init: AUC = 0.5215, AUPR=0.0102\n",
      "epoch=0, loss=0.1839, AUC=0.8059, AUPR=0.2384\n",
      "epoch=1, loss=0.1003, AUC=0.9022, AUPR=0.5644\n",
      "epoch=2, loss=0.0491, AUC=0.9366, AUPR=0.7151\n",
      "epoch=3, loss=0.0434, AUC=0.9395, AUPR=0.7050\n",
      "epoch=4, loss=0.0325, AUC=0.9228, AUPR=0.6476\n",
      "epoch=5, loss=0.0135, AUC=0.8942, AUPR=0.7014\n",
      "Init: AUC = 0.5132, AUPR=0.0116\n",
      "epoch=0, loss=0.1758, AUC=0.7580, AUPR=0.1050\n",
      "epoch=1, loss=0.0990, AUC=0.9082, AUPR=0.4907\n",
      "epoch=2, loss=0.0454, AUC=0.9388, AUPR=0.6569\n",
      "epoch=3, loss=0.0225, AUC=0.9348, AUPR=0.6470\n",
      "epoch=4, loss=0.0151, AUC=0.9165, AUPR=0.6664\n",
      "Init: AUC = 0.5059, AUPR=0.0100\n",
      "epoch=0, loss=0.1281, AUC=0.8214, AUPR=0.3347\n",
      "epoch=1, loss=0.0785, AUC=0.9306, AUPR=0.6032\n",
      "epoch=2, loss=0.0480, AUC=0.9520, AUPR=0.7080\n",
      "epoch=3, loss=0.0314, AUC=0.9365, AUPR=0.7102\n",
      "epoch=4, loss=0.0139, AUC=0.9151, AUPR=0.6843\n",
      "Init: AUC = 0.4848, AUPR=0.0088\n",
      "epoch=0, loss=0.1467, AUC=0.7884, AUPR=0.0939\n",
      "epoch=1, loss=0.0909, AUC=0.9226, AUPR=0.4917\n",
      "epoch=2, loss=0.0544, AUC=0.9471, AUPR=0.6382\n",
      "epoch=3, loss=0.0424, AUC=0.9557, AUPR=0.6452\n",
      "epoch=4, loss=0.0204, AUC=0.9390, AUPR=0.7142\n",
      "epoch=5, loss=0.0223, AUC=0.9122, AUPR=0.6941\n",
      "Init: AUC = 0.4835, AUPR=0.0096\n",
      "epoch=0, loss=0.1966, AUC=0.8107, AUPR=0.1771\n",
      "epoch=1, loss=0.0551, AUC=0.9244, AUPR=0.6256\n",
      "epoch=2, loss=0.0235, AUC=0.9639, AUPR=0.7051\n",
      "epoch=3, loss=0.0366, AUC=0.9476, AUPR=0.7310\n",
      "epoch=4, loss=0.0166, AUC=0.9250, AUPR=0.7507\n",
      "Init: AUC = 0.5147, AUPR=0.0111\n",
      "epoch=0, loss=0.1622, AUC=0.7759, AUPR=0.1907\n",
      "epoch=1, loss=0.0736, AUC=0.9085, AUPR=0.5639\n",
      "epoch=2, loss=0.0344, AUC=0.9241, AUPR=0.6255\n",
      "epoch=3, loss=0.0216, AUC=0.9236, AUPR=0.6638\n",
      "epoch=4, loss=0.0113, AUC=0.8910, AUPR=0.6825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init: AUC = 0.4796, AUPR=0.0088\n",
      "epoch=0, loss=0.1300, AUC=0.7940, AUPR=0.1035\n",
      "epoch=1, loss=0.0809, AUC=0.9296, AUPR=0.5884\n",
      "epoch=2, loss=0.0328, AUC=0.9532, AUPR=0.6919\n",
      "epoch=3, loss=0.0166, AUC=0.9200, AUPR=0.6908\n",
      "epoch=4, loss=0.0258, AUC=0.8935, AUPR=0.6892\n",
      "AUC=0.9475, AUPR=0.6734\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    data_name = 'Enzyme'# Enzyme, Ion Channel, GPCR, Nuclear Receptor\n",
    "    data_dir = 'datasets/'+ data_name + '.txt'\n",
    "    drugs_num, targets_num, data_list, drug_ids_dict, target_ids_dict = load_data(file_dir=data_dir)\n",
    "    print(data_name + ': N=%d, M=%d' %(drugs_num, targets_num))\n",
    "    \n",
    "    if data_name == 'Enzyme':\n",
    "        dg_dir = 'datasets/e_simmat_dg.txt'\n",
    "        dc_dir = 'datasets/e_simmat_dc.txt'\n",
    "    elif data_name == 'Ion Channel':\n",
    "        dg_dir = 'datasets/ic_simmat_dg.txt'\n",
    "        dc_dir = 'datasets/ic_simmat_dc.txt'\n",
    "    elif data_name == 'GPCR':\n",
    "        dg_dir = 'datasets/gpcr_simmat_dg.txt'\n",
    "        dc_dir = 'datasets/gpcr_simmat_dc.txt'\n",
    "    elif data_name == 'Nuclear Receptor':\n",
    "        dg_dir = 'datasets/nr_simmat_dg.txt'\n",
    "        dc_dir = 'datasets/nr_simmat_dc.txt'\n",
    "    \n",
    "    data_dg = pd.read_table(dg_dir,sep=\"\\t\", header=0, index_col=0)\n",
    "    data_dc = pd.read_table(dc_dir,sep=\"\\t\", header=0, index_col=0)\n",
    "    S_d = data_dg.values\n",
    "    S_t = data_dc.values\n",
    "    \n",
    "    for repeat in range(3):\n",
    "        train(data_list=data_list, S_d=S_d, S_t=S_t, drugs_num=drugs_num, targets_num=targets_num, mode='auc')\n",
    "        print('---------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### train for new drugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_list, S_d, S_t, drugs_num, targets_num, mode='auc'):\n",
    "    batch_size = 256 #batch大小\n",
    "    embedding_size = 64 # 用户的嵌入空间维度\n",
    "    hidden_size = [32,32] #隐层节点数目\n",
    "    learning_rate = 0.001 #学习率\n",
    "    lamda_regularizer = 1e-4 #正则项系数\n",
    "    lamda_regularizer_d = 1e-1 #正则项系数 \n",
    "    lamda_regularizer_t = 1e-1 #正则项系数\n",
    "    sample_size = 4\n",
    "    epochs  = 256\n",
    "    cv = 10\n",
    "    \n",
    "    # k折划分子集\n",
    "    kf = KFold(n_splits=cv, shuffle=True)\n",
    "    data_mat = sequence2mat(sequence=data_list, N=drugs_num, M=targets_num)\n",
    "    \n",
    "    cv_auc_list, cv_aupr_list = [],[]\n",
    "    for train_ids, test_ids in kf.split(range(drugs_num)):\n",
    "        instances_train = [[d,t,data_mat[d,t]] for d in train_ids for t in range(targets_num)]\n",
    "        instances_test = [[d,t,data_mat[d,t]] for d in test_ids for t in range(targets_num)]\n",
    "        \n",
    "        train_list = np.array(instances_train)\n",
    "        test_list, test_labels = np.array(instances_test)[:,:2], np.array(instances_test)[:,-1]\n",
    "        train_mat = sequence2mat(sequence=train_list, N=drugs_num, M=targets_num)# train data : user-item matrix\n",
    "            \n",
    "        #创建模型\n",
    "        model = NeuRanks(drugs_num = drugs_num,\n",
    "                         targets_num = targets_num,\n",
    "                         S_d = S_d, \n",
    "                         S_t = S_t,\n",
    "                         batch_size = batch_size,\n",
    "                         embedding_size = embedding_size,\n",
    "                         hidden_size = hidden_size,\n",
    "                         learning_rate = learning_rate,\n",
    "                         lamda_regularizer=lamda_regularizer,\n",
    "                         lamda_regularizer_d = lamda_regularizer_d,\n",
    "                         lamda_regularizer_t = lamda_regularizer_t)\n",
    "        \n",
    "        auc_score, aupr_score = model.evaluate(X=np.array(test_list), labels=test_labels)\n",
    "        print('Init: AUC = %.4f, AUPR=%.4f' %(auc_score, aupr_score))\n",
    "        \n",
    "        auc_list, aupr_list = [],[]\n",
    "        auc_list.append(auc_score)\n",
    "        aupr_list.append(aupr_score)\n",
    "        for epoch in range(epochs):\n",
    "            data_sequence = generate_data(train_mat=train_mat, sample_size=sample_size)\n",
    "            loss_records = model.train(data_sequence=data_sequence)\n",
    "            auc_score, aupr_score = model.evaluate(X=np.array(test_list), labels=test_labels)\n",
    "            auc_list.append(auc_score)\n",
    "            aupr_list.append(aupr_score)\n",
    "            print('epoch=%d, loss=%.4f, AUC=%.4f, AUPR=%.4f' %(epoch,loss_records[-1],auc_score, aupr_score))\n",
    "                \n",
    "            if mode=='auc':\n",
    "                mode_list = auc_list\n",
    "            else:\n",
    "                mode_list = aupr_list\n",
    "\n",
    "            if loss_records[-1] < 0.03 and len(mode_list) > 10 and mode_list[-3] > mode_list[-1] and mode_list[-3] > mode_list[-2]:\n",
    "                cv_auc, cv_aupr = auc_list[-3], aupr_list[-3]\n",
    "                break\n",
    "            cv_auc, cv_aupr = auc_score, aupr_score\n",
    "        cv_auc_list.append(cv_auc)\n",
    "        cv_aupr_list.append(cv_aupr)\n",
    "    \n",
    "    print('AUC=%.4f, AUPR=%.4f' %(np.mean(cv_auc_list),np.mean(cv_aupr_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train for new targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_list, S_d, S_t, drugs_num, targets_num, mode='auc'):\n",
    "    batch_size = 64 #batch大小\n",
    "    embedding_size = 64 # 用户的嵌入空间维度\n",
    "    hidden_size = [32,16] #隐层节点数目\n",
    "    learning_rate = 1e-3 #学习率\n",
    "    lamda_regularizer = 1e-5 #正则项系数\n",
    "    lamda_regularizer_d = 1e-1 #正则项系数 \n",
    "    lamda_regularizer_t = 1e-1 #正则项系数 \n",
    "    sample_size = 4\n",
    "    epochs  = 40\n",
    "    cv = 10\n",
    "    \n",
    "    # k折划分子集\n",
    "    kf = KFold(n_splits=cv, shuffle=True)\n",
    "    data_mat = sequence2mat(sequence=data_list, N=drugs_num, M=targets_num)\n",
    "    \n",
    "    cv_auc_list, cv_aupr_list = [],[]\n",
    "    for train_ids, test_ids in kf.split(range(targets_num)):\n",
    "        instances_train = [[d,t,data_mat[d,t]] for t in train_ids for d in range(drugs_num)]\n",
    "        instances_test = [[d,t,data_mat[d,t]] for t in test_ids for d in range(drugs_num)]\n",
    "        \n",
    "        train_list = np.array(instances_train)\n",
    "        test_list, test_labels = np.array(instances_test)[:,:2], np.array(instances_test)[:,-1]\n",
    "        train_mat = sequence2mat(sequence=train_list, N=drugs_num, M=targets_num)# train data : user-item matrix\n",
    "            \n",
    "        #创建模型\n",
    "        model = NeuRanks(drugs_num = drugs_num,\n",
    "                         targets_num = targets_num,\n",
    "                         S_d = S_d, \n",
    "                         S_t = S_t,\n",
    "                         batch_size = batch_size,\n",
    "                         embedding_size = embedding_size,\n",
    "                         hidden_size = hidden_size,\n",
    "                         learning_rate = learning_rate,\n",
    "                         lamda_regularizer=lamda_regularizer,\n",
    "                         lamda_regularizer_d = lamda_regularizer_d,\n",
    "                         lamda_regularizer_t = lamda_regularizer_t)\n",
    "        auc_score, aupr_score = model.evaluate(X=np.array(test_list), labels=test_labels)\n",
    "        print('Init: AUC = %.4f, AUPR=%.4f' %(auc_score, aupr_score))\n",
    "        \n",
    "        auc_list, aupr_list = [],[]\n",
    "        auc_list.append(auc_score)\n",
    "        aupr_list.append(aupr_score)\n",
    "        for epoch in range(epochs):\n",
    "            data_sequence = generate_data(train_mat=train_mat, sample_size=sample_size)\n",
    "            loss_records = model.train(data_sequence=data_sequence)\n",
    "            auc_score, aupr_score = model.evaluate(X=np.array(test_list), labels=test_labels)\n",
    "            auc_list.append(auc_score)\n",
    "            aupr_list.append(aupr_score)\n",
    "            print('epoch=%d, loss=%.4f, AUC=%.4f, AUPR=%.4f' %(epoch,loss_records[-1],auc_score, aupr_score))\n",
    "                \n",
    "            if mode=='auc':\n",
    "                mode_list = auc_list\n",
    "            else:\n",
    "                mode_list = aupr_list\n",
    "\n",
    "            #if len(mode_list)>10 and mode_list[-2] < mode_list[-3] > mode_list[-1]:\n",
    "             #   cv_auc, cv_aupr = auc_list[-3], aupr_list[-3]\n",
    "              #  break\n",
    "            cv_auc, cv_aupr = auc_score, aupr_score\n",
    "        cv_auc_list.append(cv_auc)\n",
    "        cv_aupr_list.append(cv_aupr)\n",
    "    \n",
    "    print('AUC=%.4f, AUPR=%.4f' %(np.mean(cv_auc_list),np.mean(cv_aupr_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
